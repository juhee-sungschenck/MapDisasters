{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.metrics import confusion_matrix, plot_confusion_matrix, accuracy_score, silhouette_score\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import DBSCAN, KMeans\n",
    "import nltk\n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(41101, 7)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>label</th>\n",
       "      <th>n_sentence</th>\n",
       "      <th>n_words</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-08-31 23:15:25+00:00</td>\n",
       "      <td>a customer service rep told me friday there is...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>25.525284</td>\n",
       "      <td>-80.606920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-08-31 22:39:25+00:00</td>\n",
       "      <td>tomorrow at pm after hour with sabor havana ci...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>19</td>\n",
       "      <td>27.686273</td>\n",
       "      <td>-80.934588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-08-31 22:31:09+00:00</td>\n",
       "      <td>hurricane laura wallop area with high mortgage...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>27.701712</td>\n",
       "      <td>-75.255859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-08-31 20:25:29+00:00</td>\n",
       "      <td>i never wish bad on anyone but think we need a...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>29.114762</td>\n",
       "      <td>-84.339632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-08-31 19:51:39+00:00</td>\n",
       "      <td>wth is pricemart so full their a hurricane idk...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>28.506867</td>\n",
       "      <td>-89.678090</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Date  \\\n",
       "0  2020-08-31 23:15:25+00:00   \n",
       "1  2020-08-31 22:39:25+00:00   \n",
       "2  2020-08-31 22:31:09+00:00   \n",
       "3  2020-08-31 20:25:29+00:00   \n",
       "4  2020-08-31 19:51:39+00:00   \n",
       "\n",
       "                                          text_clean  label  n_sentence  \\\n",
       "0  a customer service rep told me friday there is...      0           2   \n",
       "1  tomorrow at pm after hour with sabor havana ci...      0           2   \n",
       "2  hurricane laura wallop area with high mortgage...      0           2   \n",
       "3  i never wish bad on anyone but think we need a...      0           1   \n",
       "4  wth is pricemart so full their a hurricane idk...      0           2   \n",
       "\n",
       "   n_words        lat       long  \n",
       "0       11  25.525284 -80.606920  \n",
       "1       19  27.686273 -80.934588  \n",
       "2       13  27.701712 -75.255859  \n",
       "3        9  29.114762 -84.339632  \n",
       "4        5  28.506867 -89.678090  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in csv\n",
    "\n",
    "df = pd.read_csv('../data/final_hurricane_labeled.csv')\n",
    "\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date           0\n",
       "text_clean    25\n",
       "label          0\n",
       "n_sentence     0\n",
       "n_words        0\n",
       "lat            0\n",
       "long           0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check null values\n",
    "\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it could be due to non-english tweets that I did not know how to process in cleaning stage \n",
    "# will drop these rows\n",
    "\n",
    "df.dropna(axis = 0, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.526658\n",
       "1    0.473342\n",
       "Name: label, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define baseline accuracy\n",
    "\n",
    "df['label'].value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define labels\n",
    "my_labels = ['earthquake', 'no-earthquake']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final model: word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gensim and create a model to vectorize the words\n",
    "\n",
    "import gensim\n",
    "\n",
    "file_path = '../vectors/lexvec.enwiki+newscrawl.300d.W.pos.vectors'\n",
    "\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import stopwords\n",
    "\n",
    "from gensim.parsing.preprocessing import STOPWORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# customize stopwords using union\n",
    "\n",
    "my_stop_words = STOPWORDS.union(set(['earthquake']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## code taken from \n",
    "## https://github.com/susanli2016/NLP-with-Python/blob/master/Text%20Classification%20model%20selection.ipynb\n",
    "## and adapted\n",
    "\n",
    "model.init_sims(replace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bal',\n",
       " 'harley',\n",
       " 'proponents',\n",
       " 'escalating',\n",
       " 'madeleine',\n",
       " 'crushing',\n",
       " 'yielded',\n",
       " 'understandable',\n",
       " 'agnes',\n",
       " 'victorious',\n",
       " 'rockefeller',\n",
       " 'deeds',\n",
       " 'jude',\n",
       " 'doomed',\n",
       " 'sundays',\n",
       " 'rejecting',\n",
       " 'prep',\n",
       " 'concession',\n",
       " 'leopold',\n",
       " 'dislike']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking the vocabulary stored in gensim's word2vec vocabulary\n",
    "\n",
    "from itertools import islice\n",
    "list(islice(model.vocab, 13030, 13050))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create our own message in logging\n",
    "\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average word vectors\n",
    "\n",
    "def word_averaging(model, words):\n",
    "    all_words, mean = set(), []\n",
    "    \n",
    "    for word in words:\n",
    "        \n",
    "        # only check the words if not in  stopwords\n",
    "        if word not in my_stop_words:\n",
    "        \n",
    "            # check if the word is in the array of words in tweet, if so, append\n",
    "            if isinstance(word, np.ndarray):\n",
    "                mean.append(word)\n",
    "            \n",
    "            # if the word is also found in gensim word2vec vocabulary, append unit normalized vectors\n",
    "            # and add the index of the words in all_words\n",
    "            elif word in model.vocab:\n",
    "                mean.append(model.syn0norm[model.vocab[word].index])\n",
    "                all_words.add(model.vocab[word].index)\n",
    "    \n",
    "    # if mean returns no unit vectors, produce warning message and 0s\n",
    "    if not mean:\n",
    "        logging.warning('cannot compute similarity with no input %s', words)\n",
    "        return np.zeros(model.vector_size, )\n",
    "    \n",
    "    # calculate mean from the collected unit vectors\n",
    "    mean = gensim.matutils.unitvec(np.array(mean).mean(axis = 0)).astype(np.float32)\n",
    "    \n",
    "    return mean\n",
    "\n",
    "def word_averaging_list(model, text_list):\n",
    "    \n",
    "    # return the calculated mean from above function in a array by stacking them\n",
    "    return np.vstack([word_averaging(model, tweet) for tweet in text_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize the text with word2vec\n",
    "\n",
    "def w2v_tokenize_text(text):\n",
    "    \n",
    "    # create an empty list to store the tokenized words\n",
    "    tokens = []\n",
    "    \n",
    "    # tokenize the sentence \n",
    "    for sent in nltk.sent_tokenize(text, language = 'english'):\n",
    "        \n",
    "        # tokenize the word\n",
    "        for word in nltk.word_tokenize(sent, language = 'english'):\n",
    "            \n",
    "            # if length of each word contains less than 2 characters, ignore\n",
    "            if len(word) < 2:\n",
    "                continue\n",
    "            \n",
    "            # append each word tokens\n",
    "            tokens.append(word)\n",
    "    \n",
    "    # return tokens\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split - there's no target variable!!!\n",
    "\n",
    "train, test = train_test_split(df, test_size = 0.3, random_state = 42)\n",
    "\n",
    "# then tokenize \n",
    "\n",
    "test_tokenized = test.apply(lambda r: w2v_tokenize_text(r['text_clean']), axis = 1).values\n",
    "train_tokenized = train.apply(lambda r: w2v_tokenize_text(r['text_clean']), axis = 1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:cannot compute similarity with no input ['mattderienzo']\n",
      "WARNING:root:cannot compute similarity with no input ['murdergulls']\n",
      "WARNING:root:cannot compute similarity with no input ['provvel', 'demisso', 'do', 'mandetta']\n",
      "WARNING:root:cannot compute similarity with no input ['vendo', 'kkkkkkkk']\n",
      "WARNING:root:cannot compute similarity with no input ['yessir']\n",
      "WARNING:root:cannot compute similarity with no input ['evanflood']\n",
      "WARNING:root:cannot compute similarity with no input ['never']\n",
      "WARNING:root:cannot compute similarity with no input ['floodathon']\n",
      "WARNING:root:cannot compute similarity with no input ['endorphinmachine']\n",
      "WARNING:root:cannot compute similarity with no input ['can', 'even', 'with', 'this', 'one']\n",
      "WARNING:root:cannot compute similarity with no input ['off']\n",
      "WARNING:root:cannot compute similarity with no input ['plnao']\n",
      "WARNING:root:cannot compute similarity with no input ['angelmom', 'terarize']\n",
      "WARNING:root:cannot compute similarity with no input ['nofreefeetpics']\n",
      "WARNING:root:cannot compute similarity with no input ['it', 'is']\n",
      "WARNING:root:cannot compute similarity with no input ['tirei']\n",
      "WARNING:root:cannot compute similarity with no input ['yessir']\n",
      "WARNING:root:cannot compute similarity with no input ['and', 'there', 'you', 'are']\n",
      "WARNING:root:cannot compute similarity with no input ['goodmorning']\n",
      "WARNING:root:cannot compute similarity with no input ['simmmmmmm']\n",
      "WARNING:root:cannot compute similarity with no input ['yessir']\n",
      "WARNING:root:cannot compute similarity with no input ['metoo']\n",
      "WARNING:root:cannot compute similarity with no input ['midwesthoopssl', 'wisbbyearbook', 'nylasports', 'evanflood', 'nylawisconsin', 'hoopsnyla', 'bclay']\n",
      "WARNING:root:cannot compute similarity with no input ['officialrjdio', 'for', 'me']\n",
      "WARNING:root:cannot compute similarity with no input ['angelmom', 'terarize']\n",
      "WARNING:root:cannot compute similarity with no input ['angelmom', 'terarize']\n",
      "WARNING:root:cannot compute similarity with no input ['vanhalen', 'for', 'me']\n",
      "WARNING:root:cannot compute similarity with no input ['anotando']\n",
      "WARNING:root:cannot compute similarity with no input ['hiiiiiiiiiioooooooooo']\n",
      "WARNING:root:cannot compute similarity with no input ['that', 'she', 'did']\n",
      "WARNING:root:cannot compute similarity with no input ['it', 'just', 'can', 'go', 'on']\n"
     ]
    }
   ],
   "source": [
    "# calculate the means for us to be able to use vectors for train, and test dataset\n",
    "\n",
    "X_train_word_average = word_averaging_list(model, train_tokenized)\n",
    "X_test_word_average = word_averaging_list(model, test_tokenized)\n",
    "\n",
    "# a lot of warnings - clear the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# support vector machine\n",
    "\n",
    "svc = SVC()\n",
    "svc = svc.fit(X = X_train_word_average, y = train['label'])\n",
    "y_pred = svc.predict(X_test_word_average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9414915199220969"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_pred, test['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "   earthquake       0.92      0.97      0.95      6494\n",
      "no-earthquake       0.97      0.91      0.94      5829\n",
      "\n",
      "     accuracy                           0.94     12323\n",
      "    macro avg       0.94      0.94      0.94     12323\n",
      " weighted avg       0.94      0.94      0.94     12323\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# return f1, precision, and recall score\n",
    "\n",
    "print(classification_report(test['label'], y_pred, target_names = my_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
