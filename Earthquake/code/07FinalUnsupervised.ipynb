{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.metrics import confusion_matrix, plot_confusion_matrix, accuracy_score, silhouette_score\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import DBSCAN, KMeans\n",
    "import nltk\n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(38325, 10)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>user_name</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>n_sentence</th>\n",
       "      <th>n_words</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1241152458912673794</td>\n",
       "      <td>DarkDon_</td>\n",
       "      <td>40.738154</td>\n",
       "      <td>-112.101609</td>\n",
       "      <td>2020-03-20 23:59:40+00:00</td>\n",
       "      <td>TK Kirkland's Reaction To Earthquake's Ex-Wife...</td>\n",
       "      <td>tk kirkland s reaction to earthquake ex wife o...</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1241152416214474753</td>\n",
       "      <td>ReallyInID</td>\n",
       "      <td>42.242918</td>\n",
       "      <td>-112.103304</td>\n",
       "      <td>2020-03-20 23:59:30+00:00</td>\n",
       "      <td>That sound you hear is NOT an earthquake. It’s...</td>\n",
       "      <td>that sound you hear is not an earthquake  it s...</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1241152284827856896</td>\n",
       "      <td>folsworth</td>\n",
       "      <td>40.982162</td>\n",
       "      <td>-112.104020</td>\n",
       "      <td>2020-03-20 23:58:58+00:00</td>\n",
       "      <td>he said he stocked up on ammo cause he knew Th...</td>\n",
       "      <td>he said stocked up on ammo cause knew the big ...</td>\n",
       "      <td>2</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1241152178695405570</td>\n",
       "      <td>Colony14</td>\n",
       "      <td>42.729481</td>\n",
       "      <td>-112.107004</td>\n",
       "      <td>2020-03-20 23:58:33+00:00</td>\n",
       "      <td>If an earthquake separates CA, WA, and OR from...</td>\n",
       "      <td>if an earthquake separate ca  wa and or from t...</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1241152072818364416</td>\n",
       "      <td>MonitorSismico</td>\n",
       "      <td>41.044996</td>\n",
       "      <td>-112.103864</td>\n",
       "      <td>2020-03-20 23:58:08+00:00</td>\n",
       "      <td>#Sismo M 4 SOUTH OF BALI, #INDONESIA. 20-03-20...</td>\n",
       "      <td>sismo m  south of bali  indonesia     utc http...</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id       user_name        lat        long  \\\n",
       "0  1241152458912673794        DarkDon_  40.738154 -112.101609   \n",
       "1  1241152416214474753      ReallyInID  42.242918 -112.103304   \n",
       "2  1241152284827856896       folsworth  40.982162 -112.104020   \n",
       "3  1241152178695405570        Colony14  42.729481 -112.107004   \n",
       "4  1241152072818364416  MonitorSismico  41.044996 -112.103864   \n",
       "\n",
       "                        date  \\\n",
       "0  2020-03-20 23:59:40+00:00   \n",
       "1  2020-03-20 23:59:30+00:00   \n",
       "2  2020-03-20 23:58:58+00:00   \n",
       "3  2020-03-20 23:58:33+00:00   \n",
       "4  2020-03-20 23:58:08+00:00   \n",
       "\n",
       "                                                text  \\\n",
       "0  TK Kirkland's Reaction To Earthquake's Ex-Wife...   \n",
       "1  That sound you hear is NOT an earthquake. It’s...   \n",
       "2  he said he stocked up on ammo cause he knew Th...   \n",
       "3  If an earthquake separates CA, WA, and OR from...   \n",
       "4  #Sismo M 4 SOUTH OF BALI, #INDONESIA. 20-03-20...   \n",
       "\n",
       "                                          text_clean  n_sentence  n_words  \\\n",
       "0  tk kirkland s reaction to earthquake ex wife o...           1       17   \n",
       "1  that sound you hear is not an earthquake  it s...           2       13   \n",
       "2  he said stocked up on ammo cause knew the big ...           2       23   \n",
       "3  if an earthquake separate ca  wa and or from t...           1       23   \n",
       "4  sismo m  south of bali  indonesia     utc http...           2       13   \n",
       "\n",
       "   label  \n",
       "0      1  \n",
       "1      1  \n",
       "2      1  \n",
       "3      1  \n",
       "4      1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in csv\n",
    "\n",
    "df = pd.read_csv('/Users/juhee/Desktop/GA/lsmtmp/juhee/data/final_labeled.csv')\n",
    "\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id            0\n",
       "user_name     0\n",
       "lat           0\n",
       "long          0\n",
       "date          0\n",
       "text          0\n",
       "text_clean    0\n",
       "n_sentence    0\n",
       "n_words       0\n",
       "label         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check null values\n",
    "\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.511155\n",
       "0    0.488845\n",
       "Name: label, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define baseline accuracy\n",
    "\n",
    "df['label'].value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define labels\n",
    "my_labels = ['earthquake', 'no-earthquake']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final model: word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gensim and create a model to vectorize the words\n",
    "\n",
    "import gensim\n",
    "\n",
    "file_path = '/Users/juhee/Desktop/GA/08-week/8.05-lesson-word-vectors/lexvec.enwiki+newscrawl.300d.W.pos.vectors'\n",
    "\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import stopwords\n",
    "\n",
    "from gensim.parsing.preprocessing import STOPWORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# customize stopwords using union\n",
    "\n",
    "my_stop_words = STOPWORDS.union(set(['earthquake']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "## code taken from \n",
    "## https://github.com/susanli2016/NLP-with-Python/blob/master/Text%20Classification%20model%20selection.ipynb\n",
    "## and adapted\n",
    "\n",
    "model.init_sims(replace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bal',\n",
       " 'harley',\n",
       " 'proponents',\n",
       " 'escalating',\n",
       " 'madeleine',\n",
       " 'crushing',\n",
       " 'yielded',\n",
       " 'understandable',\n",
       " 'agnes',\n",
       " 'victorious',\n",
       " 'rockefeller',\n",
       " 'deeds',\n",
       " 'jude',\n",
       " 'doomed',\n",
       " 'sundays',\n",
       " 'rejecting',\n",
       " 'prep',\n",
       " 'concession',\n",
       " 'leopold',\n",
       " 'dislike']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking the vocabulary stored in gensim's word2vec vocabulary\n",
    "\n",
    "from itertools import islice\n",
    "list(islice(model.vocab, 13030, 13050))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create our own message in logging\n",
    "\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average word vectors\n",
    "\n",
    "def word_averaging(model, words):\n",
    "    all_words, mean = set(), []\n",
    "    \n",
    "    for word in words:\n",
    "        \n",
    "        # only check the words if not in  stopwords\n",
    "        if word not in my_stop_words:\n",
    "        \n",
    "            # check if the word is in the array of words in tweet, if so, append\n",
    "            if isinstance(word, np.ndarray):\n",
    "                mean.append(word)\n",
    "            \n",
    "            # if the word is also found in gensim word2vec vocabulary, append unit normalized vectors\n",
    "            # and add the index of the words in all_words\n",
    "            elif word in model.vocab:\n",
    "                mean.append(model.syn0norm[model.vocab[word].index])\n",
    "                all_words.add(model.vocab[word].index)\n",
    "    \n",
    "    # if mean returns no unit vectors, produce warning message and 0s\n",
    "    if not mean:\n",
    "        logging.warning('cannot compute similarity with no input %s', words)\n",
    "        return np.zeros(model.vector_size, )\n",
    "    \n",
    "    # calculate mean from the collected unit vectors\n",
    "    mean = gensim.matutils.unitvec(np.array(mean).mean(axis = 0)).astype(np.float32)\n",
    "    \n",
    "    return mean\n",
    "\n",
    "def word_averaging_list(model, text_list):\n",
    "    \n",
    "    # return the calculated mean from above function in a array by stacking them\n",
    "    return np.vstack([word_averaging(model, tweet) for tweet in text_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize the text with word2vec\n",
    "\n",
    "def w2v_tokenize_text(text):\n",
    "    \n",
    "    # create an empty list to store the tokenized words\n",
    "    tokens = []\n",
    "    \n",
    "    # tokenize the sentence \n",
    "    for sent in nltk.sent_tokenize(text, language = 'english'):\n",
    "        \n",
    "        # tokenize the word\n",
    "        for word in nltk.word_tokenize(sent, language = 'english'):\n",
    "            \n",
    "            # if length of each word contains less than 2 characters, ignore\n",
    "            if len(word) < 2:\n",
    "                continue\n",
    "            \n",
    "            # append each word tokens\n",
    "            tokens.append(word)\n",
    "    \n",
    "    # return tokens\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split - there's no target variable!!!\n",
    "\n",
    "train, test = train_test_split(df, test_size = 0.3, random_state = 42)\n",
    "\n",
    "# then tokenize \n",
    "\n",
    "test_tokenized = test.apply(lambda r: w2v_tokenize_text(r['text']), axis = 1).values\n",
    "train_tokenized = train.apply(lambda r: w2v_tokenize_text(r['text']), axis = 1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the means for us to be able to use vectors for train, and test dataset\n",
    "\n",
    "X_train_word_average = word_averaging_list(model, train_tokenized)\n",
    "X_test_word_average = word_averaging_list(model, test_tokenized)\n",
    "\n",
    "# a lot of warnings - clear the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# support vector machine\n",
    "\n",
    "svc = SVC()\n",
    "svc = svc.fit(X = X_train_word_average, y = train['label'])\n",
    "y_pred = svc.predict(X_test_word_average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8667594364237259"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_pred, test['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "   earthquake       0.85      0.89      0.87      5618\n",
      "no-earthquake       0.89      0.85      0.87      5880\n",
      "\n",
      "     accuracy                           0.87     11498\n",
      "    macro avg       0.87      0.87      0.87     11498\n",
      " weighted avg       0.87      0.87      0.87     11498\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# return f1, precision, and recall score\n",
    "\n",
    "print(classification_report(test['label'], y_pred, target_names = my_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We were able to achieve F1 score of 0.87!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
